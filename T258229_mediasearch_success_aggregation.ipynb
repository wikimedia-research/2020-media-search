{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MediaSearch Successful Searches, Positions, and Filter Usage\n",
    "\n",
    "Four additional metrics in [T258229](https://phabricator.wikimedia.org/T258229) that we want to aggregate:\n",
    "\n",
    "* What's the click-through rate (to quickview and from quickview to file pages)?\n",
    "* What's the average position of clicked result in successful searches?\n",
    "* What percentage of users click to copy the filename or wikitext to their clipboard?\n",
    "* What percentage of users use a filter during their search session?\n",
    "\n",
    "In this case I'm not going to gather data from legacy search for three reasons: 1) we already calculated a baseline for that, and 2) we'll be focused on improving MediaSearch moving forward, and 3) legacy search is a *lot* of data to sift through for little benefit.\n",
    "\n",
    "I'm also going to measure this on a per-session basis. While we might be able to measure this on a per-search basis, we're mainly interested in understanding whether users are able to find what they're looking for during a session. It might be that the run multiple searches to refine their results, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from wmfdata import spark, mariadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Timestamps\n",
    "\n",
    "We'll call the day we're gathering data for `data_day`. We're also expecting this notebook to be run the day after, which we'll call `next_day`. In order to ignore search sessions that started on the previous day, we also define that day. Lastly, we set a limit of one hour after midnight UTC as the cutoff for data. In other words, we expect search sessions to be completed within one hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_day = dt.datetime.now(dt.timezone.utc).date()\n",
    "\n",
    "data_day = next_day - dt.timedelta(days = 1)\n",
    "previous_day = data_day - dt.timedelta(days = 1)\n",
    "\n",
    "limit_timestamp = dt.datetime.combine(next_day, dt.time(hour = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Tables\n",
    "\n",
    "We define a set of tables in the Data Lake for aggregation of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_success_table = 'nettrom_sd.mediasearch_success_aggregates'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_mediasearch_table_query = '''\n",
    "CREATE TABLE {table_name} (\n",
    "    log_date DATE COMMENT \"the date of the aggregated search counts\",\n",
    "    num_mediasearch_sessions BIGINT COMMENT \"the number of MediaSearch sessions\",\n",
    "    num_used_filter BIGINT COMMENT \"number of sessions that changed at least one filter\",\n",
    "    num_result_clicked BIGINT COMMENT \"number of sessions where a result was clicked\",\n",
    "    num_filepage_clicked BIGINT COMMENT \"number of sessions with a click to a file page\",\n",
    "    num_filename_copied BIGINT COMMENT \"number of sessions with a click to copy the filename\", \n",
    "    num_wikitext_copied BIGINT COMMENT \"number of sessions with a click to copy the wikitext\",\n",
    "    num_media_played BIGINT COMMENT \"number of sessions where media was played\",\n",
    "    median_position_clicked DOUBLE COMMENT \"median position of a clicked result across all sessions\"\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(create_mediasearch_table_query.format(table_name = search_success_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_partition_statement(start_ts, end_ts, prefix = ''):\n",
    "    '''\n",
    "    This takes the two timestamps and creates a statement that selects\n",
    "    partitions based on `year`, `month`, and `day` in order to make our\n",
    "    data gathering not use excessive amounts of data. It assumes that\n",
    "    `start_ts` and `end_ts` are not more than a month apart, which should\n",
    "    be a reasonable expectation for this notebook.\n",
    "    \n",
    "    An optional prefix can be set to enable selecting partitions for\n",
    "    multiple tables with different aliases.\n",
    "    \n",
    "    :param start_ts: start timestamp\n",
    "    :type start_ts: datetime.datetime\n",
    "    \n",
    "    :param end_ts: end timestamp\n",
    "    :type end_ts: datetime.datetime\n",
    "    \n",
    "    :param prefix: prefix to use in front of partition clauses, \".\" is added automatically\n",
    "    :type prefix: str\n",
    "    '''\n",
    "    \n",
    "    if prefix:\n",
    "        prefix = f'{prefix}.' # adds \".\" after the prefix\n",
    "    \n",
    "    # there are three cases:\n",
    "    # 1: month and year are the same, output a \"BETWEEN\" statement with the days\n",
    "    # 2: months differ, but the years are the same.\n",
    "    # 3: years differ too.\n",
    "    # Case #2 and #3 can be combined, because it doesn't really matter\n",
    "    # if the years are the same in the month-selection or not.\n",
    "    \n",
    "    if start_ts.year == end_ts.year and start_ts.month == end_ts.month:\n",
    "        return(f'''{prefix}year = {start_ts.year}\n",
    "AND {prefix}month = {start_ts.month}\n",
    "AND {prefix}day BETWEEN {start_ts.day} AND {end_ts.day}''')\n",
    "    else:\n",
    "        return(f'''\n",
    "(\n",
    "    ({prefix}year = {start_ts.year}\n",
    "     AND {prefix}month = {start_ts.month}\n",
    "     AND {prefix}day >= {start_ts.day})\n",
    " OR ({prefix}year = {end_ts.year}\n",
    "     AND {prefix}month = {end_ts.month}\n",
    "     AND {prefix}day <= {end_ts.day})\n",
    ")''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Query\n",
    "\n",
    "A few notes:\n",
    "\n",
    "1. Filters can be set at any point during the search session. This leads to two potential paths: 1) the filter change took place before a search was made, and thus applies to the first search following it; and 2) the filter change happened after a search, at which point a new search automatically happens to show new results based on the filter. We cannot tell from the `search_new` event that filters applied to it, but in the query below we allow `filter_change` to occur at any point during the search session. Search sessions are based on `search_new`, and we therefore assume that filters apply to searches made. The only case we're ignoring is a user setting and then resetting filters before running a search.\n",
    "2. We've tested that `MAX()` returns `NULL` if all inputs are `NULL`, and `1` if one of the inputs is `1`. This isn't obvious from the Spark function documentation.\n",
    "3. There's extensive use of `NULL` values to make `COUNT()` easy, rather than use `SUM()` and ones and zeroes. The previous note about how `MAX()` behaves comes into play here.\n",
    "4. We've tested whether timestamps can be used to distinguish between events, and as of late March 2021 there were no difference between requiring timestamps to be different and allowing them to be equal. Once we have more data flowing in some time in April, we might want to revisit that assumption.\n",
    "5. We've verified that the number of sessions counted in this query is the same as the number of sessions counted in the other notebook that also aggregates for legacy search. The `mediasearch_sessions` subquery is exactly the same so they should match, but it's good to know that nothing else in this query messes with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mediasearch_success_query = '''\n",
    "WITH mediasearch_sessions AS ( -- all MediaSearch sessions started during the day of interest\n",
    "    SELECT\n",
    "        web_pageview_id AS session_id,\n",
    "        MIN(coalesce(dt, meta.dt)) AS session_start_dt\n",
    "    FROM event.mediawiki_mediasearch_interaction AS ms\n",
    "    WHERE {ms_partition_statement}\n",
    "    AND action = \"search_new\"\n",
    "    GROUP BY web_pageview_id\n",
    "    HAVING TO_DATE(session_start_dt) = \"{today}\"\n",
    "),\n",
    "mediasearch_filters AS ( -- used a filter in their search session\n",
    "    SELECT\n",
    "        DISTINCT web_pageview_id AS session_id,\n",
    "        1 AS used_filter\n",
    "    FROM mediasearch_sessions AS mess\n",
    "    JOIN event.mediawiki_mediasearch_interaction AS ms\n",
    "    ON mess.session_id = ms.web_pageview_id\n",
    "    WHERE {ms_partition_statement}\n",
    "    AND action = \"filter_change\"\n",
    "    AND coalesce(dt, meta.dt) < \"{limit_timestamp}\"\n",
    "),\n",
    "mediasearch_quickview AS ( -- opened quickview during a search\n",
    "    SELECT\n",
    "        web_pageview_id AS session_id,\n",
    "        MIN(coalesce(dt, meta.dt)) AS first_click_dt,\n",
    "        1 AS clicked_result\n",
    "    FROM mediasearch_sessions AS mess\n",
    "    JOIN event.mediawiki_mediasearch_interaction AS ms\n",
    "    ON mess.session_id = ms.web_pageview_id\n",
    "    WHERE {ms_partition_statement}\n",
    "    AND action = \"result_click\"\n",
    "    AND coalesce(dt, meta.dt) > session_start_dt\n",
    "    AND coalesce(dt, meta.dt) < \"{limit_timestamp}\"\n",
    "    GROUP BY web_pageview_id\n",
    "),\n",
    "mediasearch_result_actions AS (\n",
    "    -- We do all of these together because they're all based on mediasearch_quickview\n",
    "    SELECT\n",
    "        web_pageview_id AS session_id,\n",
    "        -- clicked through to a file page\n",
    "        MAX(IF(action = \"quickview_more_details_click\", 1, NULL)) AS file_page_click,\n",
    "        -- clicked to copy the filename\n",
    "        MAX(IF(action = \"quickview_filename_copy\", 1, NULL)) AS filename_copy,\n",
    "        -- clicked to copy wikitext ()\n",
    "        MAX(IF(action = \"quickview_wikitext_link_copy\", 1, NULL)) AS wikitext_copy,\n",
    "        -- played media\n",
    "        MAX(IF(action = \"quickview_media_play\", 1, NULL)) AS played_media\n",
    "    FROM mediasearch_quickview AS mq\n",
    "    JOIN event.mediawiki_mediasearch_interaction AS ms\n",
    "    ON mq.session_id = ms.web_pageview_id\n",
    "    WHERE {ms_partition_statement}\n",
    "    AND coalesce(dt, meta.dt) > mq.first_click_dt\n",
    "    AND coalesce(dt, meta.dt) < \"{limit_timestamp}\"\n",
    "    GROUP BY web_pageview_id\n",
    "),\n",
    "median_click_position AS ( -- calculate across all valid sessions and result clicks\n",
    "    SELECT\n",
    "        TO_DATE(session_start_dt) AS log_date,\n",
    "        percentile(\n",
    "            search_result_position,\n",
    "            0.5\n",
    "        ) AS median_click_position\n",
    "    FROM mediasearch_sessions AS mess\n",
    "    JOIN event.mediawiki_mediasearch_interaction AS ms\n",
    "    ON mess.session_id = ms.web_pageview_id\n",
    "    WHERE {ms_partition_statement}\n",
    "    AND action = \"result_click\"\n",
    "    AND coalesce(dt, meta.dt) > session_start_dt\n",
    "    AND coalesce(dt, meta.dt) < \"{limit_timestamp}\"\n",
    "    GROUP BY TO_DATE(session_start_dt)\n",
    "),\n",
    "mediasearch_stats AS ( -- statistics for MediaSearch\n",
    "    SELECT\n",
    "        TO_DATE(session_start_dt) AS log_date,\n",
    "        COUNT(1) AS num_mediasearch_sessions,\n",
    "        COUNT(mf.used_filter) AS num_used_filter,\n",
    "        COUNT(mq.clicked_result) AS num_result_clicked,\n",
    "        COUNT(ma.file_page_click) AS num_filepage_clicked,\n",
    "        COUNT(ma.filename_copy) AS num_filename_copied,\n",
    "        COUNT(ma.wikitext_copy) AS num_wikitext_copied,\n",
    "        COUNT(ma.played_media) AS num_media_played\n",
    "    FROM mediasearch_sessions AS mess\n",
    "    LEFT JOIN mediasearch_filters AS mf\n",
    "    ON mess.session_id = mf.session_id\n",
    "    LEFT JOIN mediasearch_quickview AS mq\n",
    "    ON mess.session_id = mq.session_id\n",
    "    LEFT JOIN mediasearch_result_actions AS ma\n",
    "    ON mess.session_id = ma.session_id\n",
    "    GROUP BY TO_DATE(session_start_dt)\n",
    ")\n",
    "INSERT INTO {aggregate_table}\n",
    "SELECT\n",
    "   mass.*,\n",
    "   coalesce(mcp.median_click_position, 0.0) AS median_position_clicked\n",
    "FROM mediasearch_stats AS mass\n",
    "LEFT JOIN median_click_position AS mcp\n",
    "ON mass.log_date = mcp.log_date\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\n",
    "    mediasearch_success_query.format(\n",
    "            today = data_day,\n",
    "            limit_timestamp = limit_timestamp.isoformat(),\n",
    "            ess_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ess'),\n",
    "            ms_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ms'),\n",
    "            aggregate_table = search_success_table\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/bin/python3.7.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.run(mediasearch_success_query.format(\n",
    "        today = data_day,\n",
    "        limit_timestamp = limit_timestamp.isoformat(),\n",
    "        ess_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ess'),\n",
    "        ms_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ms'),\n",
    "        aggregate_table = search_success_table\n",
    "    ))\n",
    "except UnboundLocalError:\n",
    "    # wmfdata currently (late Feb 2021) has an issue with DDL/DML SQL queries,\n",
    "    # and so we ignore that error\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
