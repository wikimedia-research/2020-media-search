{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation of Search Activity on Commons\n",
    "\n",
    "Per [T258229](https://phabricator.wikimedia.org/T258229), we aggregate daily measurements of search activity on Commons. In this notebook, we gather statistics for both legacy search and Special:MediaSearch. Specifically, we gather data on the following six measurements:\n",
    "\n",
    "* Number of search sessions\n",
    "* Number of searches made\n",
    "* Number of searches per session\n",
    "* Search session length\n",
    "* Click-through rate (to quickview and from quickview to file pages)\n",
    "* Average position of clicked result in successful searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from wmfdata import spark, mariadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Timestamps\n",
    "\n",
    "We'll call the day we're gathering data for `data_day`. We're also expecting this notebook to be run the day after, which we'll call `next_day`. In order to ignore search sessions that started on the previous day, we also define that day. Lastly, we set a limit of one hour after midnight UTC as the cutoff for data. In other words, we expect search sessions to be completed within one hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_day = dt.datetime.now(dt.timezone.utc).date()\n",
    "\n",
    "data_day = next_day - dt.timedelta(days = 1)\n",
    "previous_day = data_day - dt.timedelta(days = 1)\n",
    "\n",
    "limit_timestamp = dt.datetime.combine(next_day, dt.time(hour = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Tables\n",
    "\n",
    "We define a set of tables in the Data Lake for aggregation of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_count_table_name = 'nettrom_sd.commons_search_counts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_search_count_query = '''\n",
    "CREATE TABLE {table_name} (\n",
    "    log_date DATE COMMENT \"the date of the aggregated search counts\",\n",
    "    num_legacy_sessions BIGINT COMMENT \"the number of legacy search sessions\",\n",
    "    num_autocomplete_searches BIGINT COMMENT \"the number of autocomplete searches\",\n",
    "    num_fulltext_successful_searches BIGINT COMMENT \"the number of fulltext searches with results\",\n",
    "    num_fulltext_zeroresult_searches BIGINT COMMENT \"the number of fulltext searches with no results\",\n",
    "    median_autocomp_searches_per_session DOUBLE COMMENT \"median number of autocomplete searches per sesssion\",\n",
    "    median_ft_success_searches_per_session DOUBLE COMMENT \"median number of fulltext searches with results per session\",\n",
    "    median_ft_zero_searches_per_session DOUBLE COMMENT \"median number of fulltext searches with no results per session\",\n",
    "    median_legacy_session_length DOUBLE COMMENT \"median length of a search session, in seconds\",\n",
    "    num_mediasearch_sessions BIGINT COMMENT \"the number of MediaSearch search sessions\",\n",
    "    num_mediasearch_searches BIGINT COMMENT \"the number of searches made in MediaSearch sessions\",\n",
    "    median_mediasearch_searches_per_session DOUBLE COMMENT \"median number of searches, per MediaSearch session\",\n",
    "    median_mediasearch_session_length DOUBLE COMMENT \"median length of a MediaSearch session, in seconds\"\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(create_search_count_query.format(table_name = search_count_table_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_partition_statement(start_ts, end_ts, prefix = ''):\n",
    "    '''\n",
    "    This takes the two timestamps and creates a statement that selects\n",
    "    partitions based on `year`, `month`, and `day` in order to make our\n",
    "    data gathering not use excessive amounts of data. It assumes that\n",
    "    `start_ts` and `end_ts` are not more than a month apart, which should\n",
    "    be a reasonable expectation for this notebook.\n",
    "    \n",
    "    An optional prefix can be set to enable selecting partitions for\n",
    "    multiple tables with different aliases.\n",
    "    \n",
    "    :param start_ts: start timestamp\n",
    "    :type start_ts: datetime.datetime\n",
    "    \n",
    "    :param end_ts: end timestamp\n",
    "    :type end_ts: datetime.datetime\n",
    "    \n",
    "    :param prefix: prefix to use in front of partition clauses, \".\" is added automatically\n",
    "    :type prefix: str\n",
    "    '''\n",
    "    \n",
    "    if prefix:\n",
    "        prefix = f'{prefix}.' # adds \".\" after the prefix\n",
    "    \n",
    "    # there are three cases:\n",
    "    # 1: month and year are the same, output a \"BETWEEN\" statement with the days\n",
    "    # 2: months differ, but the years are the same.\n",
    "    # 3: years differ too.\n",
    "    # Case #2 and #3 can be combined, because it doesn't really matter\n",
    "    # if the years are the same in the month-selection or not.\n",
    "    \n",
    "    if start_ts.year == end_ts.year and start_ts.month == end_ts.month:\n",
    "        return(f'''{prefix}year = {start_ts.year}\n",
    "AND {prefix}month = {start_ts.month}\n",
    "AND {prefix}day BETWEEN {start_ts.day} AND {end_ts.day}''')\n",
    "    else:\n",
    "        return(f'''\n",
    "(\n",
    "    ({prefix}year = {start_ts.year}\n",
    "     AND {prefix}month = {start_ts.month}\n",
    "     AND {prefix}day >= {start_ts.day})\n",
    " OR ({prefix}year = {end_ts.year}\n",
    "     AND {prefix}month = {end_ts.month}\n",
    "     AND {prefix}day <= {end_ts.day})\n",
    ")''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of searches\n",
    "\n",
    "Note: I did a pull of a week's worth of searches on Commons and found the difference between `meta.dt` and `client_dt` to generally be very small, typically within a minute. Instead of doing time math to ignore sessions with large drifts, we'll instead coalesce the two and ignore sessions that are outside our defined time limitations, which should be a very reasonable decision.\n",
    "\n",
    "The query below limits sessions for legacy search to those having less than 50 searches in them. This is in order to focus on non-automated traffic and is an approach that's been used in search analysis in the past, for example when we gathered baseline metrics for legacy search in [T258723](https://phabricator.wikimedia.org/T258723).\n",
    "\n",
    "Note that we calculate medians (using `percentile()`) as the \"average number of searches made per session\" because we've previously found that search activity has a long-tail distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_count_query = '''\n",
    "WITH legacy_sessions AS ( -- all legacy search sessions started during the day of interest\n",
    "    SELECT\n",
    "        event.searchsessionid AS session_id,\n",
    "        MIN(coalesce(client_dt, meta.dt)) AS session_start_dt\n",
    "    FROM event.searchsatisfaction AS ess\n",
    "    WHERE {ess_partition_statement}\n",
    "    AND wiki = \"commonswiki\"\n",
    "    AND useragent.is_bot = false\n",
    "    AND event.subTest IS NULL\n",
    "    AND event.action = \"searchResultPage\"\n",
    "    AND event.isforced IS NULL -- only include non-test users\n",
    "    GROUP BY event.searchsessionid\n",
    "    HAVING TO_DATE(session_start_dt) = \"{today}\"\n",
    "),\n",
    "legacy_session_end ( -- timestamp of last event in a valid legacy search session\n",
    "    SELECT\n",
    "        event.searchsessionid AS session_id,\n",
    "        MAX(coalesce(client_dt, meta.dt)) AS session_end_dt\n",
    "    FROM legacy_sessions AS ls\n",
    "    INNER JOIN event.searchsatisfaction AS ess\n",
    "    ON ls.session_id = ess.event.searchsessionid\n",
    "    WHERE {ess_partition_statement}\n",
    "    AND event.action != \"checkin\" -- don't count page checkins where a user is reading a page\n",
    "    AND coalesce(client_dt, meta.dt) < \"{limit_timestamp}\" -- until end of our data window\n",
    "    GROUP BY event.searchsessionid\n",
    "),\n",
    "legacy_counts AS ( -- count of searches based on valid legacy search sessions\n",
    "    SELECT\n",
    "        event.searchsessionid AS session_id,\n",
    "        COUNT(DISTINCT\n",
    "                IF(event.source = \"autocomplete\", event.searchsessionid, NULL),\n",
    "                IF(event.source = \"autocomplete\", event.pageviewid, NULL)) AS num_autocomplete_searches,\n",
    "        COUNT(IF(event.source = \"fulltext\"\n",
    "            AND event.hitsReturned > 0 , 1, NULL)) AS num_fulltext_successful_searches,\n",
    "        COUNT(IF(event.source = \"fulltext\"\n",
    "            AND event.hitsReturned IS NULL , 1, NULL)) AS num_fulltext_zeroresult_searches\n",
    "    FROM legacy_sessions AS ls\n",
    "    INNER JOIN event.searchsatisfaction AS ess\n",
    "    ON ls.session_id = ess.event.searchsessionid\n",
    "    WHERE {ess_partition_statement}\n",
    "    AND coalesce(client_dt, meta.dt) < \"{limit_timestamp}\" -- until end of our data window\n",
    "    AND event.action = \"searchResultPage\"\n",
    "    GROUP BY event.searchsessionid\n",
    "),\n",
    "mediasearch_sessions AS ( -- all MediaSearch sessions started during the day of interest\n",
    "    SELECT\n",
    "        web_pageview_id AS session_id,\n",
    "        MIN(coalesce(dt, meta.dt)) AS session_start_dt\n",
    "    FROM event.mediawiki_mediasearch_interaction AS ms\n",
    "    WHERE {ms_partition_statement}\n",
    "    AND action = \"search_new\"\n",
    "    GROUP BY web_pageview_id\n",
    "    HAVING TO_DATE(session_start_dt) = \"{today}\"\n",
    "),\n",
    "mediasearch_session_end ( -- timestamp of last event in a valid Mediasearch session\n",
    "    SELECT\n",
    "        web_pageview_id AS session_id,\n",
    "        MAX(coalesce(dt, meta.dt)) AS session_end_dt\n",
    "    FROM mediasearch_sessions AS mess\n",
    "    INNER JOIN event.mediawiki_mediasearch_interaction AS ms\n",
    "    ON mess.session_id = ms.web_pageview_id\n",
    "    WHERE {ms_partition_statement}\n",
    "    AND coalesce(dt, meta.dt) < \"{limit_timestamp}\" -- until end of our data window\n",
    "    GROUP BY web_pageview_id\n",
    "),\n",
    "mediasearch_counts AS ( -- count of searches based on valid MediaSearch sessions\n",
    "    SELECT\n",
    "        web_pageview_id AS session_id,\n",
    "        COUNT(1) AS num_mediasearch_searches\n",
    "    FROM mediasearch_sessions AS mess\n",
    "    INNER JOIN event.mediawiki_mediasearch_interaction AS ms\n",
    "    ON mess.session_id = ms.web_pageview_id\n",
    "    WHERE {ms_partition_statement}\n",
    "    AND coalesce(dt, meta.dt) < \"{limit_timestamp}\" -- until end of our data window\n",
    "    AND action = \"search_new\"\n",
    "    GROUP BY web_pageview_id\n",
    "),\n",
    "legacy_stats AS ( -- statistics for legacy search\n",
    "    SELECT\n",
    "        TO_DATE(session_start_dt) AS log_date,\n",
    "        COUNT(1) AS num_legacy_sessions,\n",
    "        SUM(num_autocomplete_searches) AS num_autocomplete_searches,\n",
    "        SUM(num_fulltext_successful_searches) AS num_fulltext_successful_searches,\n",
    "        SUM(num_fulltext_zeroresult_searches) AS num_fulltext_zeroresult_searches,\n",
    "        percentile(\n",
    "            unix_timestamp(session_end_dt, \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\") -\n",
    "                unix_timestamp(session_start_dt, \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"),\n",
    "            0.5 -- median is the 50th percentile\n",
    "        ) AS median_legacy_session_length,\n",
    "        percentile(\n",
    "            num_autocomplete_searches,\n",
    "            0.5 -- median is the 50th percentile\n",
    "        ) AS median_autocomp_searches_per_session,\n",
    "        percentile(\n",
    "            num_fulltext_successful_searches,\n",
    "            0.5 -- median is the 50th percentile\n",
    "        ) AS median_ft_success_searches_per_session,\n",
    "        percentile(\n",
    "            num_fulltext_zeroresult_searches,\n",
    "            0.5 -- median is the 50th percentile\n",
    "        ) AS median_ft_zero_searches_per_session\n",
    "    FROM legacy_sessions AS ls\n",
    "    INNER JOIN legacy_session_end AS lse\n",
    "    ON ls.session_id = lse.session_id\n",
    "    INNER JOIN legacy_counts AS lc\n",
    "    ON ls.session_id = lc.session_id\n",
    "    -- cutoff of 50 searches per session to remove automated traffic\n",
    "    WHERE (num_autocomplete_searches + num_fulltext_successful_searches) < 50\n",
    "    GROUP BY TO_DATE(session_start_dt)\n",
    "),\n",
    "mediasearch_stats AS ( -- statistics for MediaSearch\n",
    "    SELECT\n",
    "        TO_DATE(session_start_dt) AS log_date,\n",
    "        COUNT(1) AS num_mediasearch_sessions,\n",
    "        SUM(num_mediasearch_searches) AS num_mediasearch_searches,\n",
    "        percentile(\n",
    "            unix_timestamp(session_end_dt, \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\") -\n",
    "                unix_timestamp(session_start_dt, \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"),\n",
    "            0.5 -- median is the 50th percentile\n",
    "        ) AS median_mediasearch_session_length,\n",
    "        percentile(\n",
    "            num_mediasearch_searches,\n",
    "            0.5 -- median is the 50th percentile\n",
    "        ) AS median_mediasearch_searches_per_session\n",
    "    FROM mediasearch_sessions AS mess\n",
    "    INNER JOIN mediasearch_session_end AS mse\n",
    "    ON mess.session_id = mse.session_id\n",
    "    INNER JOIN mediasearch_counts AS mc\n",
    "    ON mess.session_id = mc.session_id\n",
    "    GROUP BY TO_DATE(session_start_dt)\n",
    ")\n",
    "INSERT INTO {aggregate_table}\n",
    "SELECT\n",
    "    ls.log_date,\n",
    "    ls.num_legacy_sessions,\n",
    "    ls.num_autocomplete_searches,\n",
    "    ls.num_fulltext_successful_searches,\n",
    "    ls.num_fulltext_zeroresult_searches,\n",
    "    ls.median_autocomp_searches_per_session,\n",
    "    ls.median_ft_success_searches_per_session,\n",
    "    ls.median_ft_zero_searches_per_session,\n",
    "    ls.median_legacy_session_length,\n",
    "    coalesce(ms.num_mediasearch_sessions, 0) AS num_mediasearch_sessions,\n",
    "    coalesce(ms.num_mediasearch_searches, 0) AS num_mediasearch_searches,\n",
    "    coalesce(ms.median_mediasearch_searches_per_session, 0.0) AS median_mediasearch_searches_per_session,\n",
    "    coalesce(ms.median_mediasearch_session_length, 0.0) AS median_mediasearch_session_length\n",
    "FROM legacy_stats AS ls\n",
    "LEFT JOIN mediasearch_stats AS ms\n",
    "ON ls.log_date = ms.log_date\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(search_count_query.format(\n",
    "        today = data_day,\n",
    "        limit_timestamp = limit_timestamp.isoformat(),\n",
    "        ess_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ess'),\n",
    "        ms_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ms'),\n",
    "        # aggregate_table = table_name\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/bin/python3.7.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.run(search_count_query.format(\n",
    "        today = data_day,\n",
    "        limit_timestamp = limit_timestamp.isoformat(),\n",
    "        ess_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ess'),\n",
    "        ms_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ms'),\n",
    "        aggregate_table = search_count_table_name\n",
    "    ))\n",
    "except UnboundLocalError:\n",
    "    # wmfdata currently (late Feb 2021) has an issue with DDL/DML SQL queries,\n",
    "    # and so we ignore that error\n",
    "    pass\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
