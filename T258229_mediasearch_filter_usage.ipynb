{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MediaSearch Filter Usage Aggregation\n",
    "\n",
    "There are two questions about filter usage in T258229:\n",
    "\n",
    "* What percentage of users use more than 1? 2? 3? filters in their session?\n",
    "* What filters are used the most?\n",
    "\n",
    "We note that not all filters are available for all searches. For example, the namespace filter is only available when searching for categories and pages. In this case we'll not take into consideration what the potential number of filters used might be for a specific search so that we can understand to what extent users take advantage of filters. We have the \"proportion of sessions using filters\" for that. Instead, we'll aggregate across all sessions and leave interpretation of this aggregation to whomever is using them.\n",
    "\n",
    "For the first question, I interpret that as counting the number of distinct filters changed during a session. We'll then aggregate across that number and store the date, number of filters, and number of sessions.\n",
    "\n",
    "For the second question, I interpret that as counting the number of events setting a filter to a specific value. We'll aggregate that and store the date, filter type, filter value, and number of events.\n",
    "\n",
    "For simplicity, we'll create two tables, one for each aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from wmfdata import spark, mariadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring Timestamps\n",
    "\n",
    "We'll call the day we're gathering data for `data_day`. We're also expecting this notebook to be run the day after, which we'll call `next_day`. In order to ignore search sessions that started on the previous day, we also define that day. Lastly, we set a limit of one hour after midnight UTC as the cutoff for data. In other words, we expect search sessions to be completed within one hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_day = dt.datetime.now(dt.timezone.utc).date()\n",
    "\n",
    "data_day = next_day - dt.timedelta(days = 1)\n",
    "previous_day = data_day - dt.timedelta(days = 1)\n",
    "\n",
    "limit_timestamp = dt.datetime.combine(next_day, dt.time(hour = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_per_session_counts_table = 'nettrom_sd.mediasearch_filters_per_session_aggregates'\n",
    "filter_change_aggregates_table = 'nettrom_sd.mediasearch_filter_change_aggregates'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Creation Statements\n",
    "\n",
    "These are mainly for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_filters_per_session_query = f'''\n",
    "CREATE TABLE {filters_per_session_counts_table} (\n",
    "    log_date DATE COMMENT \"the date of the aggregated search counts\",\n",
    "    filter_changes INT COMMENT \"the number of filter changes in a session\",\n",
    "    num_sessions INT COMMENT \"the number of sessions with a given number of filter changes\"\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE nettrom_sd.mediasearch_filters_per_session_aggregates (\n",
      "    log_date DATE COMMENT \"the date of the aggregated search counts\",\n",
      "    filter_changes INT COMMENT \"the number of filter changes in a session\",\n",
      "    num_sessions INT COMMENT \"the number of sessions with a given number of filter changes\"\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(create_filters_per_session_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_filters_setting_query = f'''\n",
    "CREATE TABLE {filter_change_aggregates_table} (\n",
    "    log_date DATE COMMENT \"the date of the aggregated search counts\",\n",
    "    filter_type STRING COMMENT \"the type of filter set\",\n",
    "    filter_value STRING COMMENT \"the value the filter was set to\",\n",
    "    num_changes INT COMMENT \"the number of times the filter was set to that value\"\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE nettrom_sd.mediasearch_filter_change_aggregates (\n",
      "    log_date DATE COMMENT \"the date of the aggregated search counts\",\n",
      "    filter_type STRING COMMENT \"the type of filter set\",\n",
      "    filter_value STRING COMMENT \"the value the filter was set to\",\n",
      "    num_changes INT COMMENT \"the number of times the filter was set to that value\"\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(create_filters_setting_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_partition_statement(start_ts, end_ts, prefix = ''):\n",
    "    '''\n",
    "    This takes the two timestamps and creates a statement that selects\n",
    "    partitions based on `year`, `month`, and `day` in order to make our\n",
    "    data gathering not use excessive amounts of data. It assumes that\n",
    "    `start_ts` and `end_ts` are not more than a month apart, which should\n",
    "    be a reasonable expectation for this notebook.\n",
    "    \n",
    "    An optional prefix can be set to enable selecting partitions for\n",
    "    multiple tables with different aliases.\n",
    "    \n",
    "    :param start_ts: start timestamp\n",
    "    :type start_ts: datetime.datetime\n",
    "    \n",
    "    :param end_ts: end timestamp\n",
    "    :type end_ts: datetime.datetime\n",
    "    \n",
    "    :param prefix: prefix to use in front of partition clauses, \".\" is added automatically\n",
    "    :type prefix: str\n",
    "    '''\n",
    "    \n",
    "    if prefix:\n",
    "        prefix = f'{prefix}.' # adds \".\" after the prefix\n",
    "    \n",
    "    # there are three cases:\n",
    "    # 1: month and year are the same, output a \"BETWEEN\" statement with the days\n",
    "    # 2: months differ, but the years are the same.\n",
    "    # 3: years differ too.\n",
    "    # Case #2 and #3 can be combined, because it doesn't really matter\n",
    "    # if the years are the same in the month-selection or not.\n",
    "    \n",
    "    if start_ts.year == end_ts.year and start_ts.month == end_ts.month:\n",
    "        return(f'''{prefix}year = {start_ts.year}\n",
    "AND {prefix}month = {start_ts.month}\n",
    "AND {prefix}day BETWEEN {start_ts.day} AND {end_ts.day}''')\n",
    "    else:\n",
    "        return(f'''\n",
    "(\n",
    "    ({prefix}year = {start_ts.year}\n",
    "     AND {prefix}month = {start_ts.month}\n",
    "     AND {prefix}day >= {start_ts.day})\n",
    " OR ({prefix}year = {end_ts.year}\n",
    "     AND {prefix}month = {end_ts.month}\n",
    "     AND {prefix}day <= {end_ts.day})\n",
    ")''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Queries\n",
    "\n",
    "A few notes:\n",
    "\n",
    "1. The part of the queries that define a valid MediaSearch session is the same as for the other notebooks, in order to ensure consistency across metrics.\n",
    "2. Filters can be set at any point during the search session. This leads to two potential paths: 1) the filter change took place before a search was made, and thus applies to the first search following it; and 2) the filter change happened after a search, at which point a new search automatically happens to show new results based on the filter. We cannot tell from the `search_new` event that filters applied to it, but in the query below we allow `filter_change` to occur at any point during the search session. Search sessions are based on `search_new`, and we therefore assume that filters apply to searches made. The only case we're ignoring is a user setting and then resetting filters before running a search. This is also consistent with how filter-based aggregations occur in other notebooks.\n",
    "3. We do not count sessions without filters, that is handled by the other notebook that aggregates overall filter usage (proportion of sessions using at least one filter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_per_session_query = '''\n",
    "WITH mediasearch_sessions AS ( -- all MediaSearch sessions started during the day of interest\n",
    "    SELECT\n",
    "        web_pageview_id AS session_id,\n",
    "        MIN(coalesce(dt, meta.dt)) AS session_start_dt\n",
    "    FROM event.mediawiki_mediasearch_interaction AS ms\n",
    "    WHERE {ms_partition_statement}\n",
    "    AND action = \"search_new\"\n",
    "    GROUP BY web_pageview_id\n",
    "    HAVING TO_DATE(session_start_dt) = \"{today}\"\n",
    "),\n",
    "filters_per_session AS ( -- number of filter changes in each session\n",
    "    SELECT\n",
    "        TO_DATE(mess.session_start_dt) AS log_date,\n",
    "        ms.web_pageview_id AS session_id,\n",
    "        COUNT(1) AS num_filter_changes\n",
    "    FROM mediasearch_sessions AS mess\n",
    "    JOIN event.mediawiki_mediasearch_interaction AS ms\n",
    "    ON mess.session_id = ms.web_pageview_id\n",
    "    WHERE {ms_partition_statement}\n",
    "    AND action = \"filter_change\"\n",
    "    AND coalesce(dt, meta.dt) < \"{limit_timestamp}\"\n",
    "    GROUP BY TO_DATE(mess.session_start_dt), ms.web_pageview_id\n",
    ")\n",
    "INSERT INTO {aggregate_table}\n",
    "SELECT\n",
    "    FIRST_VALUE(log_date) AS log_date,\n",
    "    num_filter_changes,\n",
    "    COUNT(1) AS num_sessions\n",
    "FROM filters_per_session\n",
    "GROUP BY num_filter_changes\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\n",
    "    filters_per_session_query.format(\n",
    "            today = data_day,\n",
    "            limit_timestamp = limit_timestamp.isoformat(),\n",
    "            ms_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ms'),\n",
    "            aggregate_table = filters_per_session_counts_table\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_change_aggregate_query = '''\n",
    "WITH mediasearch_sessions AS ( -- all MediaSearch sessions started during the day of interest\n",
    "    SELECT\n",
    "        web_pageview_id AS session_id,\n",
    "        MIN(coalesce(dt, meta.dt)) AS session_start_dt\n",
    "    FROM event.mediawiki_mediasearch_interaction AS ms\n",
    "    WHERE {ms_partition_statement}\n",
    "    AND action = \"search_new\"\n",
    "    GROUP BY web_pageview_id\n",
    "    HAVING TO_DATE(session_start_dt) = \"{today}\"\n",
    "),\n",
    "mediasearch_filter_aggregates AS ( -- all filter change events\n",
    "    SELECT\n",
    "        TO_DATE(mess.session_start_dt) AS log_date,\n",
    "        search_filter_type,\n",
    "        search_filter_value,\n",
    "        COUNT(1) AS num_changes\n",
    "    FROM mediasearch_sessions AS mess\n",
    "    JOIN event.mediawiki_mediasearch_interaction AS ms\n",
    "    ON mess.session_id = ms.web_pageview_id\n",
    "    WHERE {ms_partition_statement}\n",
    "    AND action = \"filter_change\"\n",
    "    AND coalesce(dt, meta.dt) < \"{limit_timestamp}\"\n",
    "    GROUP BY TO_DATE(mess.session_start_dt), search_filter_type, search_filter_value\n",
    ")\n",
    "INSERT INTO {aggregate_table}\n",
    "SELECT\n",
    "    log_date,\n",
    "    search_filter_type,\n",
    "    IF(search_filter_value = '', 'reset', search_filter_value) AS search_filter_value,\n",
    "    num_changes\n",
    "FROM mediasearch_filter_aggregates\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\n",
    "    filter_change_aggregate_query.format(\n",
    "            today = data_day,\n",
    "            limit_timestamp = limit_timestamp.isoformat(),\n",
    "            ms_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ms'),\n",
    "            aggregate_table = filter_change_aggregates_table\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.run(filters_per_session_query.format(\n",
    "        today = data_day,\n",
    "        limit_timestamp = limit_timestamp.isoformat(),\n",
    "        ms_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ms'),\n",
    "        aggregate_table = filters_per_session_counts_table\n",
    "    ))\n",
    "except UnboundLocalError:\n",
    "    # wmfdata currently (late Feb 2021) has an issue with DDL/DML SQL queries,\n",
    "    # and so we ignore that error\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.run(filter_change_aggregate_query.format(\n",
    "        today = data_day,\n",
    "        limit_timestamp = limit_timestamp.isoformat(),\n",
    "        ms_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ms'),\n",
    "        aggregate_table = filter_change_aggregates_table\n",
    "    ))\n",
    "except UnboundLocalError:\n",
    "    # wmfdata currently (late Feb 2021) has an issue with DDL/DML SQL queries,\n",
    "    # and so we ignore that error\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
